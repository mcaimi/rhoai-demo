{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfe01c36-50d0-4b17-8c29-633d04e47e18",
   "metadata": {},
   "source": [
    "## 3 - Load Mistral Model and Transform the checkpoint\n",
    "\n",
    "Mistral is a moderately big model (we are currently using the 7B variant), so it might be hard to load it into a low-end device (e.g. a GPU with less than 16GB of VRAM)\n",
    "\n",
    "This notebook explains how to:\n",
    "* Load a full checkpoint\n",
    "* Define a quantization profile (in this case a 4-bit weight quantization)\n",
    "* Quantize the model via HuggingFace transformers using a GPU if it is available\n",
    "* Upload the resulting Quantized model to an S3 Storage for future usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9502613e-7b0f-4597-b079-7a87c8f265ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U pip\n",
    "!pip install transformers accelerate tqdm bitsandbytes sentencepiece\n",
    "!pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7848fe6a-146e-4ec5-b7b2-4de0546c1353",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import required modules\n",
    "import os, yaml\n",
    "try:\n",
    "    import sys, threading\n",
    "    import boto3\n",
    "    from boto3.s3.transfer import TransferConfig\n",
    "    from botocore.exceptions import ClientError\n",
    "    import torch\n",
    "    import torch.cuda as tc\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    from transformers import BitsAndBytesConfig\n",
    "except Exception as e:\n",
    "    print(f\"Caught Exception: {e}\")\n",
    "\n",
    "# max number of generated tokens\n",
    "max_tokens = 40\n",
    "\n",
    "# shamelessly stolen from aws docs :D\n",
    "class ProgressPercentage(object):\n",
    "    def __init__(self, filename):\n",
    "        self._filename = filename\n",
    "        self._size = float(os.path.getsize(filename))\n",
    "        self._seen_so_far = 0\n",
    "        self._lock = threading.Lock()\n",
    "\n",
    "    def __call__(self, bytes_amount):\n",
    "        # To simplify, assume this is hooked up to a single filename\n",
    "        with self._lock:\n",
    "            self._seen_so_far += bytes_amount\n",
    "            percentage = (self._seen_so_far / self._size) * 100\n",
    "            sys.stdout.write(\n",
    "                \"\\r%s  %s / %s  (%.2f%%)\" % (\n",
    "                    self._filename, self._seen_so_far, self._size,\n",
    "                    percentage))\n",
    "            sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f849400-19cd-4d2e-87c3-9515a109830b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dictionary class that holds parameters\n",
    "# load values from a yaml file\n",
    "class Parameters(object):\n",
    "    def __init__(self, data: dict):\n",
    "        if type(data) != dict:\n",
    "            raise TypeError(f\"Parameters: expected 'dict', got {type(data)}.\")\n",
    "        else:\n",
    "            self.data = data\n",
    "\n",
    "        for k in self.data.keys():\n",
    "            if type(self.data.get(k)) != dict:\n",
    "                self.__setattr__(k, self.data.get(k))\n",
    "            else:\n",
    "                self.__setattr__(k, Parameters(self.data.get(k)))\n",
    "\n",
    "# load parameters file and read values into a dictionary class\n",
    "try:\n",
    "    with open(\"parameters.yaml\") as parms:\n",
    "        config_parms = yaml.safe_load(parms)\n",
    "    creds = Parameters(config_parms)\n",
    "except yaml.YAMLError as e:\n",
    "    print(f\"Error loading YAML file: {e}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Caught exception: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c4679b-f28c-437b-a8bc-b3657de4511d",
   "metadata": {},
   "source": [
    "Download the full model from a pre-loaded storage endpoint (MinIO in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd59467b-f339-407a-b87a-fbccfd0fafef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download model from s3 if needed\n",
    "try:\n",
    "    # connect to MinIO and prepare buckets\n",
    "    print(f\"Accessing S3 endpoint {creds.params.url} with ACCESS_KEY {creds.params.accessKey}...\")\n",
    "\n",
    "    # instantiate connection\n",
    "    minio_api = boto3.client(\"s3\", endpoint_url=creds.params.url, aws_access_key_id=creds.params.accessKey, aws_secret_access_key=creds.params.secretKey)\n",
    "except Exception as e:\n",
    "    print(f\"Caught exception: {e}\")\n",
    "\n",
    "# create folder to store training data\n",
    "mistral_models_path = \"/\".join((creds.huggingface.modelsPath, creds.huggingface.modelName))\n",
    "os.makedirs(mistral_models_path, exist_ok=True)\n",
    "\n",
    "# get list of data files\n",
    "try:\n",
    "    for file in creds.huggingface.filenames:\n",
    "        if not os.path.exists(\"/\".join((mistral_models_path, file))):\n",
    "            print(f\"Downloading file: {file} to {mistral_models_path}\")\n",
    "            minio_api.download_file(creds.huggingface.modelBucket, file,\n",
    "                                        \"/\".join((mistral_models_path, file)))\n",
    "        else:\n",
    "            print(f\"File {file} already downloaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"Caught Exception {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071510bc-8861-40e6-818a-15363112af9e",
   "metadata": {},
   "source": [
    "Detect available CUDA Accelerators. Use the GPU if one is found, otherwise fallback to CPU operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67cce5d-24d0-492d-b056-ccd20a56cd12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make sure that an accelerator is attached to the pytorch runtime\n",
    "accelerator = \"cpu\"\n",
    "if tc.is_available():\n",
    "    print(f\"Torch reports: CUDA is available, {tc.device_count()} GPU\")\n",
    "    print(f\"Accelerator model: {tc.get_device_name()}\")\n",
    "    accelerator = \"cuda\"\n",
    "\n",
    "# query nvidia card via SMI\n",
    "!nvidia-smi\n",
    "\n",
    "# report detected accelerator\n",
    "print(f\"Using accelerator '{accelerator}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00137543-9281-4a9c-97a4-c602f5818a58",
   "metadata": {},
   "source": [
    "# Define a Quantization Profile\n",
    "\n",
    "Define a quantized profile (use 4bit precision instead of full precision) in order to reduce the model size\n",
    "This can easily be done with HuggingFace transformers library and the BitsAndBytes library\n",
    "\n",
    "cfr. https://huggingface.co/blog/4bit-transformers-bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dcbbd3-ebb5-41bb-abb8-c0c346ff95d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mistral is a huge model\n",
    "# we may run into trouble when converting it to ONNX since we only have 16GB of VRAM\n",
    "# so we quantize the weights of the model to reduce the size:\n",
    "# load the model with 4bit precision (1/4 size)\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927cd848-aaf1-483d-b748-443a78179821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenizer model from pretrained checkpoint\n",
    "mistral_tokenizer = AutoTokenizer.from_pretrained(mistral_models_path,\n",
    "                                                  torch_dtype=torch.float16,\n",
    "                                                  device_map=\"auto\",\n",
    "                                                  quantization_config=quantization_config,\n",
    "                                                  use_safetensors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91964091-6a2a-4b89-a0a6-3bc9ec170b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load llm from pretrained checkpoint\n",
    "mistral_model = AutoModelForCausalLM.from_pretrained(mistral_models_path,\n",
    "                                                     torch_dtype=torch.float16,\n",
    "                                                     device_map=\"auto\",\n",
    "                                                     quantization_config=quantization_config,\n",
    "                                                     use_safetensors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5adc05-89fd-4bcf-949b-e867d633da40",
   "metadata": {},
   "source": [
    "Now check the size of the resulting quantized model. Note that the size of the checkpoint has been really cut down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be33c933-6932-4fca-af0a-1374527a9961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model has been loaded\n",
    "!nvidia-smi\n",
    "print(f\"Memory footprint of the current model (4-bit quantized): {mistral_model.get_memory_footprint()/(1024*1024*1024)}GB of VRAM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997d6b0f-6dd0-4d1b-b9d6-d03e22cafcd6",
   "metadata": {},
   "source": [
    "Test inference with the resulting model to ensure that it does indeed work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce67e22-a978-488f-aa43-cf1213dc5dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test tokenizer\n",
    "input_tokens = mistral_tokenizer(\"Tell me about RedHat\", return_tensors=\"pt\").to(accelerator)\n",
    "print(input_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb398d0-f54b-49d1-80d6-f091c394f308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test inference\n",
    "output_tokens = mistral_model.generate(**input_tokens, max_new_tokens=max_tokens)\n",
    "print(output_tokens)\n",
    "\n",
    "# show generated message\n",
    "print(mistral_tokenizer.decode(output_tokens[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8e53e4-2d01-4edc-a625-6c0962c62e7f",
   "metadata": {},
   "source": [
    "# Finally upload the model to S3\n",
    "\n",
    "After quantization and test, the resulting 4-bit checkpoint is uploaded to S3 for storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc516cf1-ef10-45af-98d0-86bb44357002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data to local disk\n",
    "try:\n",
    "    # OK, model works, prepare disk path for saving\n",
    "    quantized_savepath = \"/\".join((creds.huggingface.modelsPath, \"quantized\"))\n",
    "    os.makedirs(quantized_savepath, exist_ok=True)\n",
    "\n",
    "    # save the quantized pretrained model to disk\n",
    "    mistral_model.save_pretrained(quantized_savepath)\n",
    "except Exception as e:\n",
    "    print(f\"Caught exception: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfad2499-5dbc-4b6b-8bdd-561ce0764d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload quantized model to s3\n",
    "quantized_model_files = [ \"model.safetensors\", \"generation_config.json\", \"config.json\" ]\n",
    "tokenizer_model_files = [ \"tokenizer.model\", \"tokenizer.json\", \"tokenizer_config.json\", \"special_tokens_map.json\" ]\n",
    "\n",
    "# connect to MinIO and prepare buckets\n",
    "print(f\"Accessing S3 endpoint {creds.params.url} with ACCESS_KEY {creds.params.accessKey}...\")\n",
    "\n",
    "# instantiate connection\n",
    "minio_api = boto3.client(\"s3\", endpoint_url=creds.params.url, aws_access_key_id=creds.params.accessKey, aws_secret_access_key=creds.params.secretKey)\n",
    "\n",
    "# checks whether a file exists in a remote bucket\n",
    "def check_exists(s3api, bucket, filename):\n",
    "    rsp = s3api.list_objects_v2(Bucket=bucket, Prefix=filename)\n",
    "    try:\n",
    "        contents = rsp.get(\"Contents\")\n",
    "        files = [ obj.get(\"Key\") for obj in contents ]\n",
    "        if filename in files:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "# Set the desired multipart threshold value (5GB)\n",
    "GB = 1024 ** 3\n",
    "transfer_config = TransferConfig(multipart_threshold = 5*GB, use_threads=False)\n",
    "try:\n",
    "    for k in tokenizer_model_files:\n",
    "        if not check_exists(minio_api, creds.quantized_model.quantizedBucket, k):\n",
    "            print(f\"Uploading {k} to MinIO bucket {creds.quantized_model.quantizedBucket}\")\n",
    "            filepath = \"/\".join((mistral_models_path, k))\n",
    "            minio_api.upload_file(filepath, creds.quantized_model.quantizedBucket,\n",
    "                                    k,\n",
    "                                    Callback=ProgressPercentage(filepath),\n",
    "                                    Config=transfer_config)\n",
    "            print(\"---\")\n",
    "        else:\n",
    "            print(f\"File {k} already exists in {creds.quantized_model.quantizedBucket}\")\n",
    "\n",
    "    for k in quantized_model_files:\n",
    "        if not check_exists(minio_api, creds.quantized_model.quantizedBucket, k):\n",
    "            print(f\"Uploading {k} to MinIO bucket {creds.quantized_model.quantizedBucket}\")\n",
    "            filepath = \"/\".join((quantized_savepath, k))\n",
    "            minio_api.upload_file(filepath, creds.quantized_model.quantizedBucket,\n",
    "                                    k,\n",
    "                                    Callback=ProgressPercentage(filepath),\n",
    "                                    Config=transfer_config)\n",
    "            print(\"---\")\n",
    "        else:\n",
    "            print(f\"File {k} already exists in {creds.quantized_model.quantizedBucket}\")\n",
    "except ClientError as e:\n",
    "    print(f\"S3 Exception: {e.response['Error']['Code']}, trace: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Caught exception: {e}\")\n",
    "\n",
    "print(\"Upload Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcec6ea-d63f-4c62-b6d4-4e43045419d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear vram and unload live model from gpu\n",
    "if mistral_model:\n",
    "    del mistral_model\n",
    "    tc.empty_cache()\n",
    "\n",
    "# show CUDA status\n",
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
