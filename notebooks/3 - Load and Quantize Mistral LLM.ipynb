{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9502613e-7b0f-4597-b079-7a87c8f265ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/app-root/lib/python3.9/site-packages (24.0)\n",
      "Requirement already satisfied: transformers in /opt/app-root/lib/python3.9/site-packages (4.41.2)\n",
      "Requirement already satisfied: accelerate in /opt/app-root/lib/python3.9/site-packages (0.31.0)\n",
      "Requirement already satisfied: tqdm in /opt/app-root/lib/python3.9/site-packages (4.66.4)\n",
      "Requirement already satisfied: bitsandbytes in /opt/app-root/lib/python3.9/site-packages (0.43.1)\n",
      "Requirement already satisfied: filelock in /opt/app-root/lib/python3.9/site-packages (from transformers) (3.15.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/app-root/lib/python3.9/site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/app-root/lib/python3.9/site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/app-root/lib/python3.9/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/app-root/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/app-root/lib/python3.9/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /opt/app-root/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/app-root/lib/python3.9/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/app-root/lib/python3.9/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: psutil in /opt/app-root/lib/python3.9/site-packages (from accelerate) (5.9.7)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/app-root/lib/python3.9/site-packages (from accelerate) (2.3.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/app-root/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/app-root/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/app-root/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
      "Requirement already satisfied: networkx in /opt/app-root/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/app-root/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/app-root/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/app-root/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/app-root/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/app-root/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/app-root/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/app-root/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/app-root/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/app-root/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/app-root/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/app-root/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/app-root/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.0 in /opt/app-root/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/app-root/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.40)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/app-root/lib/python3.9/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/app-root/lib/python3.9/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: boto3 in /opt/app-root/lib/python3.9/site-packages (1.26.165)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.165 in /opt/app-root/lib/python3.9/site-packages (from boto3) (1.29.165)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/app-root/lib/python3.9/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/app-root/lib/python3.9/site-packages (from boto3) (0.6.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/app-root/lib/python3.9/site-packages (from botocore<1.30.0,>=1.29.165->boto3) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/app-root/lib/python3.9/site-packages (from botocore<1.30.0,>=1.29.165->boto3) (1.26.18)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.165->boto3) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U pip\n",
    "!pip install transformers accelerate tqdm bitsandbytes\n",
    "!pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7848fe6a-146e-4ec5-b7b2-4de0546c1353",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import required modules\n",
    "import os, yaml\n",
    "try:\n",
    "    import sys, threading\n",
    "    import boto3\n",
    "    from boto3.s3.transfer import TransferConfig\n",
    "    from botocore.exceptions import ClientError\n",
    "    import torch\n",
    "    import torch.cuda as tc\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    from transformers import BitsAndBytesConfig\n",
    "except Exception as e:\n",
    "    print(f\"Caught Exception: {e}\")\n",
    "\n",
    "# max number of generated tokens\n",
    "max_tokens = 40\n",
    "\n",
    "# shamelessly stolen from aws docs :D\n",
    "class ProgressPercentage(object):\n",
    "    def __init__(self, filename):\n",
    "        self._filename = filename\n",
    "        self._size = float(os.path.getsize(filename))\n",
    "        self._seen_so_far = 0\n",
    "        self._lock = threading.Lock()\n",
    "\n",
    "    def __call__(self, bytes_amount):\n",
    "        # To simplify, assume this is hooked up to a single filename\n",
    "        with self._lock:\n",
    "            self._seen_so_far += bytes_amount\n",
    "            percentage = (self._seen_so_far / self._size) * 100\n",
    "            sys.stdout.write(\n",
    "                \"\\r%s  %s / %s  (%.2f%%)\" % (\n",
    "                    self._filename, self._seen_so_far, self._size,\n",
    "                    percentage))\n",
    "            sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f849400-19cd-4d2e-87c3-9515a109830b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dictionary class that holds parameters\n",
    "# load values from a yaml file\n",
    "class Parameters(object):\n",
    "    def __init__(self, data: dict):\n",
    "        if type(data) != dict:\n",
    "            raise TypeError(f\"Parameters: expected 'dict', got {type(data)}.\")\n",
    "        else:\n",
    "            self.data = data\n",
    "\n",
    "        for k in self.data.keys():\n",
    "            if type(self.data.get(k)) != dict:\n",
    "                self.__setattr__(k, self.data.get(k))\n",
    "            else:\n",
    "                self.__setattr__(k, Parameters(self.data.get(k)))\n",
    "\n",
    "# load parameters file and read values into a dictionary class\n",
    "try:\n",
    "    with open(\"parameters.yaml\") as parms:\n",
    "        config_parms = yaml.safe_load(parms)\n",
    "    creds = Parameters(config_parms)\n",
    "except yaml.YAMLError as e:\n",
    "    print(f\"Error loading YAML file: {e}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Caught exception: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd59467b-f339-407a-b87a-fbccfd0fafef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessing S3 endpoint http://minio-svc.minio.svc.cluster.local:9000 with ACCESS_KEY O3wC8Aoi1e46YSoJerUm...\n",
      "File model-00001-of-00003.safetensors already downloaded.\n",
      "File model-00002-of-00003.safetensors already downloaded.\n",
      "File model-00003-of-00003.safetensors already downloaded.\n",
      "File model.safetensors.index.json already downloaded.\n",
      "File config.json already downloaded.\n",
      "File params.json already downloaded.\n",
      "File tokenizer.model already downloaded.\n",
      "File tokenizer.json already downloaded.\n",
      "File tokenizer_config.json already downloaded.\n",
      "File special_tokens_map.json already downloaded.\n"
     ]
    }
   ],
   "source": [
    "# download model from s3 if needed\n",
    "try:\n",
    "    # connect to MinIO and prepare buckets\n",
    "    print(f\"Accessing S3 endpoint {creds.params.url} with ACCESS_KEY {creds.params.accessKey}...\")\n",
    "\n",
    "    # instantiate connection\n",
    "    minio_api = boto3.client(\"s3\", endpoint_url=creds.params.url, aws_access_key_id=creds.params.accessKey, aws_secret_access_key=creds.params.secretKey)\n",
    "except Exception as e:\n",
    "    print(f\"Caught exception: {e}\")\n",
    "\n",
    "# create folder to store training data\n",
    "mistral_models_path = \"/\".join((creds.huggingface.modelsPath, creds.huggingface.modelName))\n",
    "os.makedirs(mistral_models_path, exist_ok=True)\n",
    "\n",
    "# get list of data files\n",
    "try:\n",
    "    for file in creds.huggingface.filenames:\n",
    "        if not os.path.exists(\"/\".join((mistral_models_path, file))):\n",
    "            print(f\"Downloading file: {file} to {mistral_models_path}\")\n",
    "            minio_api.download_file(creds.huggingface.modelBucket, file,\n",
    "                                        \"/\".join((mistral_models_path, file)))\n",
    "        else:\n",
    "            print(f\"File {file} already downloaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"Caught Exception {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f67cce5d-24d0-492d-b056-ccd20a56cd12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch reports: CUDA is available, 1 GPU\n",
      "Accelerator model: Tesla V100-SXM2-16GB\n",
      "Thu Jun 20 11:50:32 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla V100-SXM2-16GB           On  | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   32C    P0              25W / 300W |      2MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "Using accelerator 'cuda'\n"
     ]
    }
   ],
   "source": [
    "# make sure that an accelerator is attached to the pytorch runtime\n",
    "accelerator = \"cpu\"\n",
    "if tc.is_available():\n",
    "    print(f\"Torch reports: CUDA is available, {tc.device_count()} GPU\")\n",
    "    print(f\"Accelerator model: {tc.get_device_name()}\")\n",
    "    accelerator = \"cuda\"\n",
    "\n",
    "# query nvidia card via SMI\n",
    "!nvidia-smi\n",
    "\n",
    "# report detected accelerator\n",
    "print(f\"Using accelerator '{accelerator}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17dcbbd3-ebb5-41bb-abb8-c0c346ff95d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mistral is a huge model\n",
    "# we may run into trouble when converting it to ONNX since we only have 16GB of VRAM\n",
    "# so we quantize the weights of the model to reduce the size:\n",
    "# load the model with 4bit precision (1/4 size)\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "927cd848-aaf1-483d-b748-443a78179821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenizer model from pretrained checkpoint\n",
    "mistral_tokenizer = AutoTokenizer.from_pretrained(mistral_models_path,\n",
    "                                                  torch_dtype=torch.float16,\n",
    "                                                  device_map=\"auto\",\n",
    "                                                  quantization_config=quantization_config,\n",
    "                                                  use_safetensors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91964091-6a2a-4b89-a0a6-3bc9ec170b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e00b5c2a1694c3a9e840d1ee7edf692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load llm from pretrained checkpoint\n",
    "mistral_model = AutoModelForCausalLM.from_pretrained(mistral_models_path,\n",
    "                                                     torch_dtype=torch.float16,\n",
    "                                                     device_map=\"auto\",\n",
    "                                                     quantization_config=quantization_config,\n",
    "                                                     use_safetensors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be33c933-6932-4fca-af0a-1374527a9961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jun 20 11:50:45 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla V100-SXM2-16GB           On  | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   33C    P0              41W / 300W |   5312MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n",
      "Memory footprint of the current model (4-bit quantized): 4.2505035400390625GB of VRAM\n"
     ]
    }
   ],
   "source": [
    "# model has been loaded\n",
    "!nvidia-smi\n",
    "print(f\"Memory footprint of the current model (4-bit quantized): {mistral_model.get_memory_footprint()/(1024*1024*1024)}GB of VRAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ce67e22-a978-488f-aa43-cf1213dc5dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    1, 16027,  1296,  1452,  4458, 29537,  1038]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "# test tokenizer\n",
    "input_tokens = mistral_tokenizer(\"Tell me about RedHat\", return_tensors=\"pt\").to(accelerator)\n",
    "print(input_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bb398d0-f54b-49d1-80d6-f091c394f308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1, 16027,  1296,  1452,  4458, 29537,  1038, 29491,   781,   781,\n",
      "          8284, 29537,  1038,  1117,  1032, 20254,  6791, 29491,  1429,  1117,\n",
      "          1032,  2701, 11281,  2355, 29491,  1429,  1117,  1032,  2701, 11281,\n",
      "          2355, 29491,  1429,  1117,  1032,  2701, 11281,  2355, 29491,  1429,\n",
      "          1117,  1032,  2701, 11281,  2355, 29491,  1429]], device='cuda:0')\n",
      "Tell me about RedHat.\n",
      "\n",
      "RedHat is a Linux distribution. It is a free operating system. It is a free operating system. It is a free operating system. It is a free operating system. It\n"
     ]
    }
   ],
   "source": [
    "# test inference\n",
    "output_tokens = mistral_model.generate(**input_tokens, max_new_tokens=max_tokens)\n",
    "print(output_tokens)\n",
    "\n",
    "# show generated message\n",
    "print(mistral_tokenizer.decode(output_tokens[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc516cf1-ef10-45af-98d0-86bb44357002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data to local disk\n",
    "try:\n",
    "    # OK, model works, prepare disk path for saving\n",
    "    quantized_savepath = \"/\".join((creds.huggingface.modelsPath, \"quantized\"))\n",
    "    os.makedirs(quantized_savepath, exist_ok=True)\n",
    "\n",
    "    # save the quantized pretrained model to disk\n",
    "    mistral_model.save_pretrained(quantized_savepath)\n",
    "except Exception as e:\n",
    "    print(f\"Caught exception: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfad2499-5dbc-4b6b-8bdd-561ce0764d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessing S3 endpoint http://minio-svc.minio.svc.cluster.local:9000 with ACCESS_KEY O3wC8Aoi1e46YSoJerUm...\n",
      "File tokenizer.model already exists in quantized\n",
      "File tokenizer.json already exists in quantized\n",
      "File tokenizer_config.json already exists in quantized\n",
      "File special_tokens_map.json already exists in quantized\n",
      "File model.safetensors already exists in quantized\n",
      "File generation_config.json already exists in quantized\n",
      "File config.json already exists in quantized\n",
      "Upload Complete.\n"
     ]
    }
   ],
   "source": [
    "# upload quantized model to s3\n",
    "quantized_model_files = [ \"model.safetensors\", \"generation_config.json\", \"config.json\" ]\n",
    "tokenizer_model_files = [ \"tokenizer.model\", \"tokenizer.json\", \"tokenizer_config.json\", \"special_tokens_map.json\" ]\n",
    "\n",
    "# connect to MinIO and prepare buckets\n",
    "print(f\"Accessing S3 endpoint {creds.params.url} with ACCESS_KEY {creds.params.accessKey}...\")\n",
    "\n",
    "# instantiate connection\n",
    "minio_api = boto3.client(\"s3\", endpoint_url=creds.params.url, aws_access_key_id=creds.params.accessKey, aws_secret_access_key=creds.params.secretKey)\n",
    "\n",
    "# checks whether a file exists in a remote bucket\n",
    "def check_exists(s3api, bucket, filename):\n",
    "    rsp = s3api.list_objects_v2(Bucket=bucket, Prefix=filename)\n",
    "    try:\n",
    "        contents = rsp.get(\"Contents\")\n",
    "        files = [ obj.get(\"Key\") for obj in contents ]\n",
    "        if filename in files:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "# Set the desired multipart threshold value (5GB)\n",
    "GB = 1024 ** 3\n",
    "transfer_config = TransferConfig(multipart_threshold = 5*GB, use_threads=True)\n",
    "try:\n",
    "    for k in tokenizer_model_files:\n",
    "        if not check_exists(minio_api, creds.quantized_model.quantizedBucket, k):\n",
    "            print(f\"Uploading {k} to MinIO bucket {creds.quantized_model.quantizedBucket}\")\n",
    "            filepath = \"/\".join((mistral_models_path, k))\n",
    "            minio_api.upload_file(filepath, creds.quantized_model.quantizedBucket,\n",
    "                                    k,\n",
    "                                    Callback=ProgressPercentage(filepath),\n",
    "                                    Config=transfer_config)\n",
    "            print(\"---\")\n",
    "        else:\n",
    "            print(f\"File {k} already exists in {creds.quantized_model.quantizedBucket}\")\n",
    "\n",
    "    for k in quantized_model_files:\n",
    "        if not check_exists(minio_api, creds.quantized_model.quantizedBucket, k):\n",
    "            print(f\"Uploading {k} to MinIO bucket {creds.quantized_model.quantizedBucket}\")\n",
    "            filepath = \"/\".join((quantized_savepath, k))\n",
    "            minio_api.upload_file(filepath, creds.quantized_model.quantizedBucket,\n",
    "                                    k,\n",
    "                                    Callback=ProgressPercentage(filepath),\n",
    "                                    Config=transfer_config)\n",
    "            print(\"---\")\n",
    "        else:\n",
    "            print(f\"File {k} already exists in {creds.quantized_model.quantizedBucket}\")\n",
    "except ClientError as e:\n",
    "    print(f\"S3 Exception: {e.response['Error']['Code']}, trace: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Caught exception: {e}\")\n",
    "\n",
    "print(\"Upload Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bcec6ea-d63f-4c62-b6d4-4e43045419d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jun 20 11:51:30 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla V100-SXM2-16GB           On  | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   34C    P0              41W / 300W |   5346MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "#clear vram and unload live model from gpu\n",
    "if mistral_model:\n",
    "    del mistral_model\n",
    "    tc.empty_cache()\n",
    "\n",
    "# show CUDA status\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623c3f3a-b61c-405b-ae09-05e61cc6d76b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
