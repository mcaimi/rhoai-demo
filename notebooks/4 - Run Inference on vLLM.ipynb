{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f2e3afa-e3a1-4884-ba71-5a59f4ba1405",
   "metadata": {},
   "source": [
    "## 4 - Test remote inference with vLLM locally\n",
    "\n",
    "Try to load the model and serve it via locally run vLLM interface. No model server is needed in this case, everything runs inside Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ba74fe-4d24-455b-b64c-38bc1f846a69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U pip\n",
    "!pip install boto3 vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eacfa8b-18c7-46b2-87b2-419973492f1f",
   "metadata": {},
   "source": [
    "Import required libraries:\n",
    "\n",
    "- vLLM is used to perform local offline inference\n",
    "- from vLLM we import the local LLM class that is used to load an HuggingFace transformer checkpoint\n",
    "- also import the SamplingParams class that is used to customize inference parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fecedd1-564d-4b07-8761-72eb43793174",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, yaml\n",
    "try:\n",
    "    import boto3\n",
    "    import torch.cuda as tc\n",
    "    from vllm import LLM, SamplingParams\n",
    "except Exception as e:\n",
    "    print(f\"Caught exception: {e}\")\n",
    "    sys.exit(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030c2129-604c-4689-acd1-92a256cd98f5",
   "metadata": {},
   "source": [
    "Load parameters from the configuration yaml file\n",
    "\n",
    "We'll need\n",
    "\n",
    "- the checkpoint path\n",
    "- the inference parameter configuration\n",
    "- checkpoint file names\n",
    "- S3 repo data connection pointers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7979002-05b5-4fa5-9603-44000a663a9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dictionary class that holds parameters\n",
    "# load values from a yaml file\n",
    "class Parameters(object):\n",
    "    def __init__(self, data: dict):\n",
    "        if type(data) != dict:\n",
    "            raise TypeError(f\"Parameters: expected 'dict', got {type(data)}.\")\n",
    "        else:\n",
    "            self.data = data\n",
    "\n",
    "        for k in self.data.keys():\n",
    "            if type(self.data.get(k)) != dict:\n",
    "                self.__setattr__(k, self.data.get(k))\n",
    "            else:\n",
    "                self.__setattr__(k, Parameters(self.data.get(k)))\n",
    "\n",
    "# load parameters file and read values into a dictionary class\n",
    "try:\n",
    "    with open(\"parameters.yaml\") as parms:\n",
    "        config_parms = yaml.safe_load(parms)\n",
    "    creds = Parameters(config_parms)\n",
    "except yaml.YAMLError as e:\n",
    "    print(f\"Error loading YAML file: {e}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Caught exception: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162e20a5-e21b-4283-a135-0bb0e17e1701",
   "metadata": {},
   "source": [
    "Download the required model checkpoint locally from the relevant S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6251655-1455-4c2c-ae9a-ce47f5d2e86b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# shamelessly stolen from aws docs :D\n",
    "class ProgressPercentage(object):\n",
    "    def __init__(self, filename):\n",
    "        self._filename = filename\n",
    "        self._size = float(os.path.getsize(filename))\n",
    "        self._seen_so_far = 0\n",
    "        self._lock = threading.Lock()\n",
    "\n",
    "    def __call__(self, bytes_amount):\n",
    "        # To simplify, assume this is hooked up to a single filename\n",
    "        with self._lock:\n",
    "            self._seen_so_far += bytes_amount\n",
    "            percentage = (self._seen_so_far / self._size) * 100\n",
    "            sys.stdout.write(\n",
    "                \"\\r%s  %s / %s  (%.2f%%)\" % (\n",
    "                    self._filename, self._seen_so_far, self._size,\n",
    "                    percentage))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "# download model from s3 if needed\n",
    "try:\n",
    "    # connect to MinIO and prepare buckets\n",
    "    print(f\"Accessing S3 endpoint {creds.params.url} with ACCESS_KEY {creds.params.accessKey}...\")\n",
    "\n",
    "    # instantiate connection\n",
    "    minio_api = boto3.client(\"s3\", endpoint_url=creds.params.url, aws_access_key_id=creds.params.accessKey, aws_secret_access_key=creds.params.secretKey)\n",
    "except Exception as e:\n",
    "    print(f\"Caught exception: {e}\")\n",
    "\n",
    "# create folder to store training data\n",
    "mistral_models_path = \"/\".join((creds.huggingface.modelsPath, creds.huggingface.modelName))\n",
    "os.makedirs(mistral_models_path, exist_ok=True)\n",
    "\n",
    "# get list of data files\n",
    "try:\n",
    "    for file in creds.huggingface.filenames:\n",
    "        if not os.path.exists(\"/\".join((mistral_models_path, file))):\n",
    "            print(f\"Downloading file: {file} to {mistral_models_path}\")\n",
    "            minio_api.download_file(creds.huggingface.modelBucket, file,\n",
    "                                        \"/\".join((mistral_models_path, file)))\n",
    "        else:\n",
    "            print(f\"File {file} already downloaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"Caught Exception {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3841ed7-73c4-40b7-8ca8-5c941a1bb1d4",
   "metadata": {},
   "source": [
    "Now let's instantiate the model from the downloaded checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5141c3b3-081d-4205-80d2-4f29efd75437",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load model and serve via vLLM\n",
    "!nvidia-smi\n",
    "# clear GPU mem, start from a clean situation\n",
    "tc.empty_cache()\n",
    "\n",
    "# load the model in vLLM\n",
    "llm_model = LLM(model=mistral_models_path, max_model_len=creds.local_inference.vllm_model_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7f2946-fe41-4766-857f-6c7f8c7c990d",
   "metadata": {},
   "source": [
    "Once we have a model loaded & ready, let's prepare for inference\n",
    "\n",
    "- set up a SamplingParams object set up with correct parameters\n",
    "- perform inference\n",
    "- translate back tokens to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3a3e48-b177-4a3e-be3b-2b8ba98ef39a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# display responses\n",
    "def response(llm_output: list) -> str:\n",
    "    for o in llm_output:\n",
    "        prompt = o.prompt\n",
    "        gen_text = o.outputs[0].text\n",
    "        print(f\"Prompt: {prompt!r},\\n Generated text: {gen_text!r}\")\n",
    "\n",
    "# prepare parameters\n",
    "sp = SamplingParams(min_tokens=creds.local_inference.min_tokens,\n",
    "                        max_tokens=creds.local_inference.max_tokens,\n",
    "                        temperature=creds.local_inference.temperature)\n",
    "\n",
    "# test inference\n",
    "prompt=\"Tell me about Red Hat Openshift\"\n",
    "responses = llm_model.generate(prompts=prompt, sampling_params=sp)\n",
    "\n",
    "# display responses\n",
    "response(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8be7d75-52e2-421f-8f38-0320bd0bfd8f",
   "metadata": {},
   "source": [
    "Finally, free up resources allocated to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd188659-973c-49af-86b7-20e0c541b171",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# free resources\n",
    "if llm_model:\n",
    "    del llm_model\n",
    "tc.empty_cache()\n",
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
